\newpage
\section{附录}

\subsection{迁移学习相关的期刊和会议}

迁移学习仍然是一个蓬勃发展的研究领域。最近几年，在顶级期刊和会议上，越来越多的研究者开始发表文章，不断提出迁移学习的新方法，不断开拓迁移学习的新应用领域。

在这里，我们对迁移学习相关的国际期刊和会议作一小结，以方便初学者寻找合适的论文。这些期刊会议可以在表~\ref{tb-appendix-journalconference}中找到。

\begin{table}[htbp]
	\centering
	\caption{迁移学习相关的期刊和会议}
	\label{tb-appendix-journalconference}
	\resizebox{1\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{序号} & \textbf{简称} & \textbf{全称} & \textbf{领域} \\ \hline \hline
		\multicolumn{4}{|c|}{国际期刊} \\ \hline
		1 & JMLR & Journal of Machine Learning Research & 机器学习 \\ \hline
		2 & MLJ & Machine Learning Journal & 机器学习 \\ \hline
		3 & AIJ & Artificial Intelligence Journal & 人工智能 \\ \hline
		4 & TKDE & IEEE Transactions on Knowledge and Data Engineering & 数据挖掘 \\ \hline
		5 & TIST & ACM Transactions on Intelligent Science and Technology & 数据挖掘 \\ \hline
		6 & PAMI & IEEE Transactions on Pattern Analysis and Machine Intelligence & 计算机视觉 \\ \hline
		7 & IJCV & International Journal of Computer Vision & 计算机视觉 \\ \hline
		8 & TIP & IEEE Transactions on Image Processing & 计算机视觉 \\ \hline
		9 & PR & Pattern Recognition & 模式识别 \\ \hline
		10 & PRL & Pattern Recognition Letters & 模式识别 \\ \hline \hline
		\multicolumn{4}{|c|}{国际会议} \\ \hline
		1 & ICML & International Conference on Machine Learning & 机器学习 \\ \hline
		2 & NIPS & Annual Conference on Neural Information Processing System & 机器学习 \\ \hline
		3 & IJCAI & International Joint Conference on Artificial Intelligence & 人工智能 \\ \hline
		4 & AAAI & AAAI conference on Artificial Intelligence & 人工智能 \\ \hline
		5 & KDD & \begin{tabular}[c]{@{}c@{}}ACM SIGKDD\\   Conference on Knowledge Discovery and Data Mining\end{tabular} & 数据挖掘 \\ \hline
		6 & ICDM & IEEE International Conference on Data Mining & 数据挖掘 \\ \hline
		7 & CVPR & \begin{tabular}[c]{@{}c@{}}IEEE Conference on Computer Vision and\\   Pattern Recognition\end{tabular} & 计算机视觉 \\ \hline
		8 & ICCV & IEEE International Conference on Computer Vision & 计算机视觉 \\ \hline
		9 & ECCV & European Conference on Computer Vision & 计算机视觉 \\ \hline
		10 & WWW & International World Wide Web Conferences & 文本、互联网 \\ \hline
		11 & CIKM & International Conference on Information and Knowledge Management & 文本分析 \\ \hline
		12 & ACMMM & ACM International Conference on Multimedia & 多媒体 \\ \hline
	\end{tabular}
}
\end{table}

\subsection{迁移学习研究学者}

这里列出了一些迁移学习领域代表性学者以及他们的最具代表性的工作，以供分享。

一般这些工作都是由他们一作，或者是由自己的学生做出来的。当然，这里所列的文章比起这些大牛发过的文章会少得多，这里仅仅列出他们最知名的工作。本部分开源在了Github~\footnote{\url{https://github.com/jindongwang/transferlearning/blob/master/doc/scholar_TL.md}}，会一直有更新，欢迎补充！

\textbf{应用研究}

\textbf{1. \href{http://www.cs.ust.hk/~qyang/}{Qiang Yang} @ HKUST}

迁移学习领域权威大牛。他所在的课题组基本都做迁移学习方面的研究。迁移学习综述《A survey on transfer learning》就出自杨强老师课题组。他的学生们：

\textbf{1). \href{http://www.cs.ust.hk/~qyang/}{Sinno J. Pan} @ NTU}

现为老师，详细介绍见第二条。

\textbf{2). \href{https://sites.google.com/view/btan/home}{Ben Tan}}

主要研究传递迁移学习(transitive transfer learning)，现在腾讯做高级研究员。代表文章：

Transitive Transfer Learning. KDD 2015.

Distant Domain Transfer Learning. AAAI 2017.

\textbf{3).  \href{https://scholar.google.com/citations?user=Ks81aO0AAAAJ&hl=zh-CN&oi=ao}{Derek Hao Hu}}

主要研究迁移学习与行为识别结合，目前在Snap公司。代表文章：

Transfer Learning for Activity Recognition via Sensor Mapping. IJCAI 2011.

Cross-domain activity recognition via transfer learning. PMC 2011.

Bridging domains using world wide knowledge for transfer learning. TKDE 2010.

\textbf{4). \href{https://sites.google.com/site/vincentwzheng/}{Vencent Wencheng Zheng}}

也做行为识别与迁移学习的结合，目前在新加坡一个研究所当研究科学家。代表文章：

User-dependent Aspect Model for Collaborative Activity Recognition. IJCAI 2011.

Transfer Learning by Reusing Structured Knowledge. AI Magazine.

Transferring Multi-device Localization Models using Latent Multi-task Learning. AAAI 2008.

Transferring Localization Models Over Time. AAAI 2008.

Cross-Domain Activity Recognition. Ubicomp 2009.

Collaborative Location and Activity Recommendations with GPS History Data. WWW 2010.

\textbf{5). Ying Wei}

做迁移学习与数据挖掘相关的研究。代表工作：

Instilling Social to Physical: Co-Regularized Heterogeneous Transfer Learning. AAAI 2016.

Transfer Knowledge between Cities. KDD 2016.

Learning to Transfer. arXiv 2017.

其他还有很多学生都做迁移学习方面的研究，更多请参考杨强老师主页。

\textbf{2. \href{http://www.cs.ust.hk/~qyang/}{Sinno J. Pan} @ NTU}

杨强老师学生，比较著名的工作是TCA方法。现在在NTU当老师，一直都在做迁移学习研究。代表工作：

A Survey On Transfer Learning. TKDE 2010. [最著名的综述]

Domain Adaptation via Transfer Component Analysis. TNNLS 2011. [著名的TCA方法]

Cross-domain sentiment classification via spectral feature alignment. WWW 2010. [著名的SFA方法]

Transferring Localization Models across Space. AAAI 2008.


\textbf{3. \href{http://www.lxduan.info/}{Lixin Duan} @ UESTC}

毕业于NTU，现在在UESTC当老师。代表工作：

Domain Transfer Multiple Kernel Learning. PAMI 2012.

Visual Event Recognition in Videos by Learning from Web Data. PAMI 2012.


\textbf{4. \href{http://ise.thss.tsinghua.edu.cn/~mlong/}{Mingsheng Long} @ THU}

毕业于清华大学，现在在清华大学当老师，一直在做迁移学习方面的工作。代表工作：

Dual Transfer Learning. SDM 2012.

Transfer Feature Learning with Joint Distribution Adaptation. ICCV 2013.

Transfer Joint Matching for Unsupervised Domain Adaptation. CVPR 2014.

Learning transferable features with deep adaptation networks. ICML 2015. [著名的DAN方法]

Deep Transfer Learning with Joint Adaptation Networks. ICML 2017.


\textbf{5. \href{http://people.eecs.berkeley.edu/~jhoffman/}{Judy Hoffman} @ UC Berkeley \& Stanford}

Feifei Li的博士后，现在当老师。她有个学生叫做Eric Tzeng，做深度迁移学习。代表工作：

Simultaneous Deep Transfer Across Domains and Tasks. ICCV 2015.

Deep Domain Confusion: Maximizing for Domain Invariance. arXiv 2014.

Adversarial Discriminative Domain Adaptation. CVPR 2017.


\textbf{6. \href{http://www.intsci.ac.cn/users/zhuangfuzhen/}{Fuzhen Zhuang} @ ICT, CAS}

中科院计算所当老师，主要做迁移学习与文本结合的研究。代表工作：

Transfer Learning from Multiple Source Domains via Consensus Regularization. CIKM 2008.


\textbf{7. \href{https://www.cs.cornell.edu/~kilian/}{Kilian Q. Weinberger} @ Cornell U.}

现在康奈尔大学当老师。\href{http://www.cse.wustl.edu/~mchen/}{Minmin Chen}是他的学生。代表工作：

Distance metric learning for large margin nearest neighbor classification. JMLR 2009.

Feature hashing for large scale multitask learning. ICML 2009.

An introduction to nonlinear dimensionality reduction by maximum variance unfolding. AAAI 2006. [著名的MVU方法]

Co-training for domain adaptation. NIPS 2011. [著名的Co-training方法]

\textbf{8. \href{http://www.cse.wustl.edu/~mchen/}{Fei Sha} @ USC}

USC教授。他曾经的学生\href{http://www.cecs.ucf.edu/faculty/boqing-gong/}{Boqing Gong}提出了著名的GFK方法。代表工作：

Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation. ICML 2013.

Geodesic flow kernel for unsupervised domain adaptation. CVPR 2012. [著名的GFK方法]


\textbf{9. Mahsa Baktashmotlagh @ U. Queensland}

现在当老师。主要做流形学习与domain adaptation结合。代表工作：

Unsupervised Domain Adaptation by Domain Invariant Projection. ICCV 2013.

Domain Adaptation on the Statistical Manifold. CVPR 2014.

Distribution-Matching Embedding for Visual Domain Adaptation. JMLR 2016.


\textbf{10. \href{https://www.microsoft.com/en-us/research/people/baochens/}{Baochen Sun} @ Microsoft}

现在在微软。著名的CoRAL系列方法的作者。代表工作：

Return of Frustratingly Easy Domain Adaptation. AAAI 2016.

Deep coral: Correlation alignment for deep domain adaptation. ECCV 2016.


\textbf{11. Wenyuan Dai}

著名的第四范式创始人，虽然不做研究了，但是当年求学时几篇迁移学习文章至今都很高引。代表工作：

Boosting for transfer learning. ICML 2007. [著名的TrAdaboost方法]

Self-taught clustering. ICML 2008.


\textbf{理论研究}

\textbf{1. \href{http://www.gatsby.ucl.ac.uk/~gretton/}{Arthur Gretton} @ UCL}

主要做two-sample test。代表工作：

A Kernel Two-Sample Test. JMLR 2013.

Optimal kernel choice for large-scale two-sample tests. NIPS 2012. [著名的MK-MMD]


\textbf{2. \href{https://cs.uwaterloo.ca/~shai/}{Shai Ben-David} @ U.Waterloo}

很多迁移学习的理论工作由他给出。代表工作：

Analysis of representations for domain adaptation. NIPS 2007.

A theory of learning from different domains. Machine Learning 2010.


\textbf{3. \href{https://alex.smola.org/}{Alex Smola} @ CMU}

做一些机器学习的理论工作，和上面两位合作比较多。代表工作非常多，不列了。

\textbf{4. \href{https://alex.smola.org/}{John Blitzer} @ Google}

著名的SCL方法提出者，现在也在做机器学习。代表工作：

Domain adaptation with structural correspondence learning. ECML 2007. [著名的SCL方法]


\textbf{5. \href{http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html}{Yoshua Bengio} @ U.Montreal}

深度学习领军人物，主要做深度迁移学习的一些理论工作。代表工作：

Deep Learning of Representations for Unsupervised and Transfer Learning. ICML 2012.

How transferable are features in deep neural networks? NIPS 2014.

Unsupervised and Transfer Learning Challenge: a Deep Learning Approach. ICML 2012.


\textbf{6. \href{http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html}{Geoffrey Hinton} @ U.Toronto}

深度学习领军人物，也做深度迁移学习的理论工作。

Distilling the knowledge in a neural network. NIPS 2014.

\subsection{迁移学习资源汇总}

\begin{itemize}
	\item (可能是有史以来)最全的迁移学习资料库，(文章/资料/代码/数据):	\url{https://github.com/jindongwang/transferlearning}
	\item 迁移学习视频教程:	\url{https://www.youtube.com/watch?v=qD6iD4TFsdQ}
	\item 知乎专栏“机器有颗玻璃心”中《小王爱迁移》系列:	\url{https://zhuanlan.zhihu.com/p/27336930},用浅显易懂的语言深入讲解经典+最新的迁移学习文章
	\item 迁移学习与领域自适应论文分享与笔记	Paperweekly：\url{http://www.paperweekly.site/collections/231/papers}
	\item 迁移学习与领域自适应公开数据集
	\url{https://github.com/jindongwang/transferlearning/blob/master/doc/dataset.md}
\end{itemize}

\subsection{迁移学习常用算法及数据资源}
\label{sec-dataset}

为了研究及测试算法性能，收集了若干来自图像、文本、及时间序列的公开数据集。并且，为了与自己所研究的算法进行对比，收集了领域内若干较前沿的算法及其相关代码，以便后期进行算法的比较测试。

\textbf{数据集：}

表~\ref{tb-dataset}收集了迁移学习领域常用的数据集。这些数据集的详细介绍和下载地址，在Github~\footnote{\url{https://github.com/jindongwang/transferlearning/blob/master/data/dataset.md}}上可以找到。我们还在Benchmark~\footnote{\url{https://github.com/jindongwang/transferlearning/blob/master/data/benchmark.md}}上提供了一些常用算法的实验结果。

\begin{table}[htbp]
	\centering
	\caption{迁移学习相关的图像、文本、和时间序列数据集统计信息}
	\label{tb-dataset}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{序号} & \textbf{数据集} & \textbf{类型} & \textbf{样本数} & \textbf{特征数} & \textbf{类别数} \\ \hline \hline
		1 & USPS & 字符识别 & 1800 & 256 & 10 \\ \hline
		2 & MNIST & 字符识别 & 2000 & 256 & 10 \\ \hline
		3 & PIE & 人脸识别 & 11554 & 1024 & 68 \\ \hline
		4 & COIL20 & 对象识别 & 1440 & 1024 & 20 \\ \hline
		5 & Office+Caltech & 对象识别 & 2533 & 800 & 10 \\ \hline
		6 & ImageNet & 图像分类 & 7341 & 4096 & 5 \\ \hline
		7 & VOC2007 & 图像分类 & 3376 & 4096 & 5 \\ \hline
		8 & LabelMe & 图像分类 & 2656 & 4096 & 5 \\ \hline
		9 & SUN09 & 图像分类 & 3282 & 4096 & 5 \\ \hline
		10 & Caltech101 & 图像分类 & 1415 & 4096 & 5 \\ \hline
		11 & 20newsgroup & 文本分类 & 25804 & / & 6 \\ \hline
		12 & Reuters-21578 & 文本分类 & 4771 & / & 3 \\ \hline
		13 & OPPORTUNITY & 行为识别 & 701366 & 27 & 4 \\ \hline
		14 & DSADS & 行为识别 & 2844868 & 27 & 19 \\ \hline
		15 & PAMAP2 & 行为识别 & 1140000 & 27 & 18 \\ \hline
	\end{tabular}
\end{table}

\textbf{各数据集的简要介绍：}

\textbf{1. 手写体识别图像数据集}

MNIST和USPS是两个通用的手写体识别数据集，它们被广泛地应用于机器学习算法评测的各个方面。USPS数据集包括7,291张训练图片和2,007张测试图片，图片大小为16$\times$16。MNIST数据集包括60,000张训练图片和10,000张测试图片，图片大小28$\times$28。USPS和MNIST数据集分别服从显著不同的概率分布，两个数据集都包含10个类别，每个类别是1-10之间的某个字符。为了构造迁移学习人物，在USPS中随机选取1,800张图片作为辅助数据、在MNIST中随机选取2,000张图片作为目标数据。交换辅助领域和目标领域可以得到另一个分类任务MNIST vs USPS。图片预处理包括:将所有图片大小线性缩放为16$\times$16，每幅图片用256维的特征向量表征，编码了图片的像素灰度值信息。辅助领域和目标领域共享特征空间和类别空间，但数据分布显著不同。

\textbf{2. 人脸识别图像数据集}

PIE代表“朝向、光照、表情”的英文单词首字母，该数据集是人脸识别的基准测试集，包括68个不同人物的41,368幅人脸照片，图片大小为32$\times$32，每个人物的照片由13个同步的相机(不同朝向)、21个不同曝光程度拍摄。简单起见，实验中采用PIE的预处理集，包括2个不同子集PIE1和PIE2，是从正面朝向的人脸照片集合(C27)中按照不同的光照和曝光条件随机选出。按如下方法构造分类任务PIEI vs PIE2：将PIE1作为辅助领域、PIE2作为目标领域;交换辅助领域和目标领域可以得到分类任务PIE2 vs PIEI。这样，辅助领域和目标领域分别由不同光照、曝光条件的人脸照片组成，从而服从显著不同的概率分布。

\textbf{3. 对象识别数据集}

COIL20包含20个对象类别共1,440张图片;每个对象类别包括72张图片，每张图片拍摄时对象水平旋转5度(共360度)。每幅图片大小为32$\times$32，表征为1,024维的向量。实验中将该数据集划分为两个不相交的子集COIL1和COIL2：COIL1包括位于拍摄角度为$[0\textdegree,85\textdegree]\cup[180\textdegree,265\textdegree]$(第一、三象限)的所有图片；COIL2包括位于拍摄角度为$[90\textdegree,175\textdegree]\cup[270\textdegree,355\textdegree]$(第二、四象限)的所有图片。这样，子集COIL1和COIL2的图片因为拍摄角度不同而服从不同的概率分布。将COIL1作为辅助领域、COIL2作为目标领域，可以构造跨领域分类任务COIL1 vs COIL2；交换辅助领域和目标领域，可以得到另外一个分类任务COIL2 vs COIL1。

Office是视觉迁移学习的主流基准数据集，包含3个对象领域Amazon(在线电商图片)、Webcam(网络摄像头拍摄的低解析度图片)、DSLR(单反相机拍摄的高解析度图片)，共有4,652张图片31个类别标签。Caltech-256是对象识别的基准数据集，包括1个对象领域Caltech，共有30,607张图片256个类别标签。对每张图片抽取SURF特征，并向量化为800维的直方图表征，所有直方图向量都进行减均值除方差的归一化处理，直方图码表由K均值聚类算法在Amazon子集上生成。具体共有4个领域C(Caltech-256), A(Amazon), W(Webcam)和D(DSLR)，从中随机选取2个不同的领域作为辅助领域和目标领域，则可构造$4 \times 3 = 12$个跨领域视觉对象识别任务，如$A \rightarrow D, A \rightarrow C, \cdots, C \rightarrow W$。

\textbf{4. 大规模图像分类数据集}

大规模图像分类数据集包含了来自5个域的图像数据：ImageNet、VOC 2007、SUN、LabelMe、以及Caltech。它们包含5个类别的图像数据：鸟，猫，椅子，狗，人。对于每个域的数据，均使用DeCaf~\cite{donahue2014decaf}进行特征提取，并取第6层的特征作为实验使用，简称DeCaf6特征。每个样本有4096个维度。

\textbf{5. 通用文本分类数据集}

20-Newsgroups数据集包含约20,000个文档，4个大类分别为comp, rec,sci和talk，每个大类包含4个子类，详细信息如表2.2所示。在实验中构造了6组跨领域二分类任务，每组任务由4个大类中随机选取2个大类构成，一个大类记为正例，另一个大类记为负例，6个任务组具体为comp vs rec, comp vs sci, comp vs talk,  rec vs sci,  rec vs talk和、sci vs talk。每个跨领域分类任务(包括辅助领域和目标领域)按如下方法生成:每个任务组p VS的两个大类p和Q分别包含4个子类P1、P2、P3、P4和Q1、Q2、Q3、Q4;随机选取p的两个子类(如P1、P2)与Q的两个子类(如Q1、Q2)构成辅助领域，其余子类(P的P3和P4和Q3和Q4构成目标领域。以上构造策略既保证辅助领域和目标领域是相关的，因为它们都来自同样的大类;又保证辅助领域和目标领域是不同的，因为它们来自不同的子类。每个任务组P VS Q可以生成36个分类任务，总计6个任务组共生成6$\times$36 = 216个分类任务。数据集经过文本预处理后包含25,804个词项特征和15,033个文档，每个文档由tf-idf向量表征。

Reuters-21578是一个较难的文本数据集，包含多个大类和子类。其中最大3个大类为orgs, people和place，可构造6个跨领域文本分类任务orgs vs people,people vs orgs, orgs vs place, place vs orgs, people vs place和place vs people。

\textbf{6. 行为识别公开数据集}

行为识别是典型的时间序列分类任务。为了测试算法在时间序列任务上的性能，收集了3个公开的行为识别数据集：OPPORTUNITY、DASDS和PAMAP2。OPPORTUNITY数据集包含4个用户在智能家居中的多种不同层次的行为。DAADS数据集包含8个人的19种日常行为。PAMAP2数据集包含9个人的18种日常生活行为。所有数据集均包括加速度计、陀螺仪和磁力计三种运动传感器。

\textbf{算法：}

收集的数据表征、迁移学习等相关领域的基准算法如表~\ref{tb-existing_algo}所示。我们还在持续更新的Github~\footnote{\url{https://github.com/jindongwang/transferlearning}}上提供了各种算法的实现代码。

\begin{table}[htbp]
	\centering
	\caption{收集的公开算法}
	\label{tb-existing_algo}
	\resizebox{1\textwidth}{!}{
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{序号} & \textbf{算法简称} & \textbf{算法全称} & \multicolumn{1}{c|}{\textbf{出处}} \\ \hline \hline
		1 & PCA & principal component analysis & \cite{fodor2002survey}  \\ \hline
		2 & KPCA & kernel principal component analysis & \cite{fodor2002survey} \\ \hline
		3 & TCA & transfer component analysis & \cite{pan2011domain} \\ \hline
		4 & GFK & geodesic flow kernel & \cite{gong2012geodesic} \\ \hline
		5 & TKL & transfer kernel learning & \cite{long2015domain} \\ \hline
		6 & TSL & transfer subspace learning & \cite{si2010bregman} \\ \hline
		7 & JDA & joint distribution adaptation & \cite{long2013transfer} \\ \hline
		8 & TJM & transfer joint matching & \cite{long2014transfer} \\ \hline
		9 & DAN & deep adaptation network & \cite{long2015learning} \\ \hline
		10 & JAN & joint adaptation network & \cite{long2017deep} \\ \hline
		11 & DTMKL & domain transfer multiple kernel learning & \cite{duan2012domain} \\ \hline
		12 & JGSA & joint geometrical and statistical adaptation & \cite{zhang2017joint} \\ \hline
		13 & SCA & scatter component analysis & \cite{ghifary2017scatter} \\ \hline
		14 & ARTL & adaptation regularization & \cite{long2014adaptation} \\ \hline
		15 & TrAdaBoost & transfer learning based adaboost & \cite{dai2007boosting} \\ \hline
		16 & GNMF & graph regularized NMF & \cite{cai2011graph} \\ \hline
		17 & CORAL & Correlation Alignment & \cite{sun2016return} \\ \hline
		18 & SDA & Subspace Distribution Alignment & \cite{sun2015subspace} \\ \hline
	\end{tabular}
}
\end{table}
